# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data on people who were targeted by the bank's marketing, such as their age, job, and what day of the week they were contacted. We seek to predict the column `y`, which I believe is whether the marketing campaign was successful in contacting/influencing the customer.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was the AutoML model using the VotingEnsemble with an accuracy of 0.9179.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

There are 3 major steps when configuring the pipeline: cleaning the data, hyperparameter tuning, and choosing our classification algorithm.
In our provided `train.py` file, we transform many columns so that they are binary instead of string (e.g. `yes = 1`, `no = 0`). Next, we choose our hyperparameter for our experiment. I used Random Sampling with regularization and the maximum number of interations to converge. Lastly, we use Logistic rregression because it is best suited for our binary classification (remember that our target variable `y` is binary. When we run this model, our primary goal is to maximize the accuracy of predicting `y`, which gave us 0.9059.

**What are the benefits of the parameter sampler you chose?**

Random sampling allows us to have similar accuracy to grid sampling, but in less time since it only takes a random sample of data to search. 
During the tuning, I chose small regularization range of values between 0.05 and 0.5 because the regularization will be stronger. I choxse to have the maximum iterations to be a random choice of 5, 10, 50, 100. This allows our model to have a variety of configurations to choose from in attempt to find the best one. 

**What are the benefits of the early stopping policy you chose?**

An early stopping policy is necessary to end the experiment when our current results have yielded successful results and the subsequent runs are not increasing the metric significantly. This bandit policy checks after every iteration whether the current metric, in this case, accuracy, is 10% higher than our best past run's accuracy. This ensures that we do not waste time and resources when we have already found a good-performing model.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The best model generated by AutoML was one that used the VotingEnsemble with an accuracy of 0.9179. Its three most important features were duration, nr.employed, and cons.conf.idx.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Both models performed similarly in terms of accuracy. The AutoML model was higher than the HyperDrive by 0.012. If we want to choose one based off on time, the HyperDrive model would be the way to go since it took about half the time to run the experiment and still yield successful results.

However, we also must consider that the AutoML takes into account a variety of algorithms and models before settling on VotingEnsemble, whereas the HyperDrive was provisioned to use logistic regression. AutoML ensures that we have the best model and is preferred ofer HyperDrive.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

For the AutoML model, there is imbalanced data (our target column has 32950 values but only 11% of them = 1) which could lead to a falsely perceived possitive effect of our model's accuracy. We could avoid this by potentially choosing a different metric, such as AUC, or gathering more data. We can also undersample or oversample the data. These changes will get rid of the imbalance and help us refine the model. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

![compute-delete](https://user-images.githubusercontent.com/71798045/223868473-3c7f7b82-da48-4319-ad3d-e802c6a13da2.PNG)
